{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping \n",
    "Web scraping is the process of gathering information from the Internet. Some websites don’t like it when automatic scrapers gather their data, while others don’t mind.\n",
    "\n",
    "Generally involves 2 steps:\n",
    "- `requests` library for retrieving content from a webpage\n",
    "- `bs4` (BeautifulSoup) for extracting the relevant information\n",
    "    - we create a Beautiful Soup object from the content that is returned and parse it using several methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_Download a Webpage\n",
    "The first thing we’ll need to do to scrape a web page is to download the page. \n",
    "The requests library will make a `GET` request to a web server, which will download the HTML contents of a given web page for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the GET request to a url\n",
    "URL = 'https://www.capitalone.ca/credit-cards/guaranteed-mastercard/?filter=all'\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can deconstruct the above URL into two main parts:\n",
    "- The base URL represents the path to the search functionality of the website: https://www.capitalone.ca/credit-cards/guaranteed-mastercard/.\n",
    "- The query parameters `?filter=all` represents additional values that can be declared on the page\n",
    "\n",
    "This code performs an HTTP request to the given URL. It retrieves the HTML data that the server sends back and stores that data in a Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our request, we get a Response object. This object has a status_code property, which indicates if the page was downloaded successfully: A status_code of 200 means that the page downloaded successfully!\n",
    "- a status code starting with a 2 generally indicates success\n",
    "- a code starting with a 4 or a 5 indicates an error\n",
    "\n",
    "Specifically:\n",
    "- 200 — everything went okay, and the result has been returned (if any)\n",
    "- 301 — the server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint name is changed.\n",
    "- 401 — the server thinks you’re not authenticated. This happens when you don’t send the right credentials to access an API (we’ll talk about authentication in a later post).\n",
    "- 400 — the server thinks you made a bad request. This can happen when you don’t send along the right data, among other things.\n",
    "- 403 — the resource you’re trying to access is forbidden — you don’t have the right permissions to see it.\n",
    "- 404 — the resource you tried to access wasn’t found on the server.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Sends a GET request.\n",
       "\n",
       ":param url: URL for the new :class:`Request` object.\n",
       ":param params: (optional) Dictionary, list of tuples or bytes to send\n",
       "    in the query string for the :class:`Request`.\n",
       ":param \\*\\*kwargs: Optional arguments that ``request`` takes.\n",
       ":return: :class:`Response <Response>` object\n",
       ":rtype: requests.Response\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/h1b/lib/python3.7/site-packages/requests/api.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look into all attributes of the page  \n",
    "# look into requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Sends a GET request.\n",
       "\n",
       ":param url: URL for the new :class:`Request` object.\n",
       ":param params: (optional) Dictionary, list of tuples or bytes to send\n",
       "    in the query string for the :class:`Request`.\n",
       ":param \\*\\*kwargs: Optional arguments that ``request`` takes.\n",
       ":return: :class:`Response <Response>` object\n",
       ":rtype: requests.Response\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/h1b/lib/python3.7/site-packages/requests/api.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "requests.get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_Query parameters\n",
    "\n",
    "The ISS Pass endpoint returns when the ISS will next pass over a given location on earth. In order to compute this, we need to pass the coordinates of the location to the API. We can do this by adding an optional keyword argument `params` to our request. In this case, there are two parameters we need to pass:\n",
    "- lat — The latitude of the location we want.\n",
    "- lon — The longitude of the location we want.\n",
    "\n",
    "We can make a dictionary with these parameters, and then pass them into the `requests.get` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page2 = requests.get(\"http://api.open-notify.org/iss-pass\")\n",
    "page2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page2 = requests.get(\"http://api.open-notify.org/iss-pass.json\")\n",
    "page2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the parameters we want to pass to the API.\n",
    "# This is the latitude and longitude of New York City.\n",
    "location_dict = {\"lat\": 40.71, \"lon\": -74}\n",
    "\n",
    "# Make a get request with the parameters.\n",
    "page3 = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=location_dict)\n",
    "page3.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page3.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'message': 'success', 'request': {'altitude': 100, 'datetime': 1595605325, 'latitude': 40.71, 'longitude': -74.0, 'passes': 5}, 'response': [{'duration': 604, 'risetime': 1595625283}, {'duration': 646, 'risetime': 1595631058}, {'duration': 577, 'risetime': 1595636939}, {'duration': 571, 'risetime': 1595642812}, {'duration': 641, 'risetime': 1595648627}]}\n"
     ]
    }
   ],
   "source": [
    "# Get the response data as a python object. Verify that it's a dictionary.\n",
    "data = page3.json()\n",
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page3.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve successfully scraped some HTML from the Internet, but when you look at it now, it just seems like a huge mess. There are tons of HTML elements here and there, thousands of attributes scattered around—and wasn’t there some JavaScript mixed in as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_Beautiful_Soup\n",
    "Beautiful Soup is a Python library for parsing structured data. \n",
    "\n",
    "It allows you to interact with HTML in a similar way to how you would interact with a web page using developer tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we’re creating a `Beautiful Soup object` that takes the HTML content you scraped earlier as its input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without bs4 ...\n",
    "page4 = requests.get('https://h1bdata.info/index.php?em=Capital+One+Services+Llc&job=Data&city=&year=2020')\n",
    "soup = BeautifulSoup(page4.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Elements by ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an HTML web page, every element can have an id attribute assigned. As the name already suggests, that id attribute makes the element uniquely identifiable on the page. You can begin to parse your page by selecting a specific element by its ID. Beautiful Soup allows you to find that specific element easily by its ID.\n",
    "\n",
    "For easier viewing, you can `.prettify()` any Beautiful Soup object when you print it out. If you call this method on the results variable that you just assigned above, then you should see all the HTML contained within the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"alert alert-success alert-dismissible\" id=\"joblink\" role=\"alert\" style=\"display:none;\">\n",
      " <button aria-label=\"Close\" class=\"close\" data-dismiss=\"alert\" type=\"button\">\n",
      "  <span aria-hidden=\"true\">\n",
      "   ×\n",
      "  </span>\n",
      " </button>\n",
      " <div align=\"left\" style=\"font-size:18px;\">\n",
      "  We have found\n",
      "  <a href=\"http://www.indeed.com/jobs?q=company:capital one services llc title:Data&amp;l=&amp;indpubnum=7749215865220997\" onclick=\"ga('send', 'event', 'Indeed Link', 'to-list-page', 'company:capital one services llc title:Data');\" rel=\"nofollow\" target=\"indeed_search\">\n",
      "   job openings\n",
      "  </a>\n",
      "  of\n",
      "  <b>\n",
      "   Data\n",
      "  </b>\n",
      "  job from\n",
      "  <b>\n",
      "   Capital One Services Llc\n",
      "  </b>\n",
      "  .\n",
      " </div>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = soup.find(id='joblink')\n",
    "print(results.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Elements by Class Name and Text Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Extracts a list of Tag objects that match the given\n",
       "criteria.  You can specify the name of the Tag and any\n",
       "attributes you want the Tag to have.\n",
       "\n",
       "The value of a key-value pair in the 'attrs' map can be a\n",
       "string, a list of strings, a regular expression object, or a\n",
       "callable that takes a string and returns whether or not the\n",
       "string matches for some custom definition of 'matches'. The\n",
       "same is true of the tag name.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/h1b/lib/python3.7/site-packages/bs4/element.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soup.find_all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2 = soup.find_all('tr')\n",
    "len(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr><th>EMPLOYER</th><th>JOB TITLE</th><th>BASE SALARY</th><th>LOCATION</th><th data-date-format=\"mm/dd/yy\">SUBMIT DATE</th><th data-date-format=\"mm/dd/yy\">START DATE</th><th>CASE STATUS</th></tr>,\n",
       " <tr><td><a href=\"index.php?em=CAPITAL+ONE+SERVICES+LLC&amp;job=DATA&amp;city=&amp;year=2020\">CAPITAL ONE SERVICES LLC</a></td><td><a href=\"index.php?em=CAPITAL+ONE+SERVICES+LLC&amp;job=DATA+ANALYSIS+MANAGER&amp;city=&amp;year=2020\">DATA ANALYSIS MANAGER</a></td><td>75,171</td><td><a href=\"index.php?em=CAPITAL+ONE+SERVICES+LLC&amp;job=DATA&amp;city=RICHMOND&amp;year=2020\">RICHMOND, VIRGINIA</a></td><td>02/25/2020</td><td>06/23/2020</td><td>CERTIFIED</td></tr>,\n",
       " <tr><td><a href=\"index.php?em=CAPITAL+ONE+SERVICES+LLC&amp;job=DATA&amp;city=&amp;year=2020\">CAPITAL ONE SERVICES LLC</a></td><td><a href=\"index.php?em=CAPITAL+ONE+SERVICES+LLC&amp;job=DATA+ANALYSIS+MANAGER&amp;city=&amp;year=2020\">DATA ANALYSIS MANAGER</a></td><td>75,171</td><td><a href=\"index.php?em=CAPITAL+ONE+SERVICES+LLC&amp;job=DATA&amp;city=RICHMOND&amp;year=2020\">RICHMOND, VIRGINIA</a></td><td>03/02/2020</td><td>08/31/2020</td><td>CERTIFIED</td></tr>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CAPITAL ONE SERVICES LLCDATA ANALYSIS MANAGER75,171RICHMOND, VIRGINIA02/25/202006/23/2020CERTIFIED'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://h1bdata.info/index.php?em=Capital+One+Services+Llc&job=Data&city=&year=All+Years',\n",
    "         'https://h1bdata.info/index.php?em=Capital+One+Services+Llc&job=Business&city=&year=All+Years', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape table data from each of the above links and store in a list\n",
    "\n",
    "all_data = []\n",
    "for link in links:\n",
    "    page_link = link\n",
    "    page_response = requests.get(page_link, timeout=1000)\n",
    "    page_content = BeautifulSoup(page_response.content, 'lxml')\n",
    "\n",
    "    # save data \n",
    "    for row in page_content.find_all('tr')[1:]:\n",
    "        row_data = []\n",
    "        for i in row:\n",
    "            row_data.append(i.text)\n",
    "        all_data.append(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CAPITAL ONE SERVICES LLC',\n",
       "  'DATA ANALYSIS MANAGER',\n",
       "  '64,355',\n",
       "  'PLANO, TEXAS',\n",
       "  '11/22/2019',\n",
       "  '12/02/2019',\n",
       "  'CERTIFIED'],\n",
       " ['CAPITAL ONE SERVICES LLC',\n",
       "  'DATA ANALYSIS MANAGER',\n",
       "  '64,355',\n",
       "  'PLANO, TEXAS',\n",
       "  '11/22/2019',\n",
       "  '12/02/2019',\n",
       "  'CERTIFIED'],\n",
       " ['CAPITAL ONE SERVICES LLC',\n",
       "  'DATA ANALYSIS MANAGER',\n",
       "  '64,355',\n",
       "  'PLANO, TEXAS',\n",
       "  '11/22/2019',\n",
       "  '12/02/2019',\n",
       "  'CERTIFIED']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorials\n",
    "- Space ISS example: https://www.dataquest.io/blog/python-api-tutorial/\n",
    "- bs4 tutorial: https://realpython.com/beautiful-soup-web-scraper-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've successfully downloaded a webpage in HTML and be able to render the content in the page. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h1b",
   "language": "python",
   "name": "h1b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
